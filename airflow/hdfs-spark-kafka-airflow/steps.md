# ðŸš€ Installation et Configuration d'Apache Airflow avec Docker

Ce document dÃ©crit comment installer et exÃ©cuter **Apache Airflow** avec **Docker Compose**, en intÃ©grant l'installation automatique des providers via `requirements.txt`.

---

## ðŸ“Œ 1. Structure du projet
CrÃ©ez un dossier `airflow-docker/` et ajoutez les fichiers suivants :
```
airflow-docker/
â”‚â”€â”€ dags/                    # Stockage des DAGs
â”‚â”€â”€ logs/                    # Logs dâ€™Airflow
â”‚â”€â”€ plugins/                 # Plugins personnalisÃ©s
â”‚â”€â”€ requirements.txt         # Fichier des dÃ©pendances Airflow
â”‚â”€â”€ docker-compose.yml       # Configuration Docker
â”‚â”€â”€ Dockerfile               # Construction de lâ€™image Airflow
â”‚â”€â”€ .env                     # Variables dâ€™environnement
```

---

## ðŸ“Œ 2. `requirements.txt` (Liste des providers)
Ajoutez les providers nÃ©cessaires :
```txt
apache-airflow-providers-apache-kafka
apache-airflow-providers-apache-spark
apache-airflow-providers-apache-hdfs
apache-airflow-providers-apache-hadoop
```
Ajoutez d'autres selon vos besoins.

---

## ðŸ“Œ 3. `Dockerfile` pour Airflow
CrÃ©ez un `Dockerfile` pour installer les providers automatiquement :
```dockerfile
FROM apache/airflow:2.7.3  # Remplacez par la derniÃ¨re version d'Airflow

# Passer Ã  lâ€™utilisateur root pour installer des dÃ©pendances
USER root
RUN apt-get update && apt-get install -y curl vim

# Copier et installer les packages Python
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# Revenir Ã  lâ€™utilisateur Airflow
USER airflow
```

---

## ðŸ“Œ 4. `docker-compose.yml`
Fichier complet pour exÃ©cuter Airflow avec **PostgreSQL**, **Kafka**, **Spark** et **HDFS** comme backend.

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:13
    container_name: airflow_postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    restart: always
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"

  spark:
    image: bitnami/spark:latest
    container_name: spark
    restart: always
    environment:
      SPARK_MODE: master
    ports:
      - "7077:7077"
      - "8081:8081"

  hdfs:
    image: bde2020/hadoop-namenode:latest
    container_name: hdfs
    restart: always
    environment:
      CLUSTER_NAME: "hadoop_cluster"
    ports:
      - "9870:9870"
    volumes:
      - hdfs_data:/hadoop/dfs/name

  airflow-webserver:
    build: .
    container_name: airflow_webserver
    restart: always
    depends_on:
      - postgres
      - kafka
      - spark
      - hdfs
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: ["webserver"]
  
  airflow-scheduler:
    build: .
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: ["scheduler"]

volumes:
  postgres_data:
  hdfs_data:
```

---

## ðŸ“Œ 5. `.env` (Facultatif)
Stockez vos variables dâ€™environnement ici :
```env
AIRFLOW_UID=50000
AIRFLOW_GID=0
```
Airflow utilisera ces valeurs pour dÃ©finir les permissions.

---

## ðŸ“Œ 6. Initialisation et Lancement
ExÃ©cutez ces commandes :
```sh
# CrÃ©er les dossiers nÃ©cessaires
mkdir -p dags logs plugins

# Initialiser la base de donnÃ©es Airflow
docker-compose up airflow-webserver airflow-scheduler -d
docker-compose run airflow-webserver airflow db init

# CrÃ©er un utilisateur admin
docker-compose run airflow-webserver airflow users create \
    --username admin --password admin \
    --firstname Air --lastname Flow \
    --role Admin --email admin@example.com

# Lancer Airflow
docker-compose up -d
```

---

## ðŸš€ Et aprÃ¨s ?
âœ… **Airflow est maintenant accessible sur** `http://localhost:8080`  
âœ… **Les providers sont installÃ©s automatiquement** au dÃ©marrage  
âœ… **Kafka, Spark et HDFS sont intÃ©grÃ©s**  

